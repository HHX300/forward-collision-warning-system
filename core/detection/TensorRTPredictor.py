import cv2
import tensorrt as trt
import pycuda.driver as cuda
cuda.init()
device = cuda.Device(0)
# import pycuda.autoinit # åˆ›å»ºäº†éšå¼å…¨å±€ context

import os
import torch
import numpy as np
import time
from typing import Tuple
from core.utils.infer_script import scale_boxes, non_max_suppression, yolo_visualize
from core.utils.trt_infer_utils import preprocess, visualize
from core.utils.nms_numpy import non_max_suppression_numpy


class TensorRTPredictor:
    def __init__(self, engine_path: str):
        """åˆå§‹åŒ–TensorRTé¢„æµ‹å™¨ï¼ˆæ˜¾å­˜åˆ†é…åœ¨åˆå§‹åŒ–é˜¶æ®µå®Œæˆï¼‰"""

        # æ¯ä¸€ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±æ˜¾å¼çš„cuda context
        self.cuda_context = device.make_context()

        self.logger = trt.Logger(trt.Logger.VERBOSE)
        self.engine = self._load_engine(engine_path)
        self.context = self.engine.create_execution_context(strategy=trt.ExecutionContextAllocationStrategy.STATIC)

        # è·å–è¾“å…¥è¾“å‡ºtensoråç§°
        self.input_name = self.engine.get_tensor_name(0)
        self.output_name = self.engine.get_tensor_name(1)

        # è·å–è¾“å…¥è¾“å‡ºå½¢çŠ¶
        self.input_shape = tuple(self.engine.get_tensor_shape(self.input_name))
        self.output_shape = tuple(self.engine.get_tensor_shape(self.output_name))

        # é¢„åˆ†é…æ˜¾å­˜
        self._setup_buffers()

        # åˆ›å»ºå›ºå®šæµ
        self.stream = cuda.Stream()
        self.is_warmed_up = False

    def _load_engine(self, engine_path: str) -> trt.ICudaEngine:
        """åŠ è½½TensorRTå¼•æ“"""
        load_start_time = time.time()
        with open(engine_path, "rb") as f, trt.Runtime(self.logger) as runtime:
            engine = runtime.deserialize_cuda_engine(f.read())
        load_end_time = time.time()
        load_time = (load_end_time - load_start_time) * 1000
        print(f"åŠ è½½å¼•æ“æ—¶é—´: {load_time:.2f} ms")
        return engine

    def _setup_buffers(self):
        """é¢„åˆ†é…è¾“å…¥è¾“å‡ºæ˜¾å­˜"""
        # è®¡ç®—ç¼“å†²åŒºå¤§å°
        input_size = int(np.prod(self.input_shape)) * np.float32().itemsize
        output_size = int(np.prod(self.output_shape)) * np.float32().itemsize

        # åˆ†é…å›ºå®šæ˜¾å­˜
        self.d_input = cuda.mem_alloc(input_size)
        self.d_output = cuda.mem_alloc(output_size)

        # é¢„åˆ†é…ä¸»æœºé”é¡µå†…å­˜
        self.h_output = cuda.pagelocked_empty(self.output_shape, dtype=np.float32)

        # è®¾ç½®tensoråœ°å€
        self.context.set_tensor_address(self.input_name, int(self.d_input))
        self.context.set_tensor_address(self.output_name, int(self.d_output))

    def warmup(self, iterations: int = 10):
        """æ¨¡å‹é¢„çƒ­ï¼ˆä½¿ç”¨é¢„åˆ†é…æ˜¾å­˜ï¼‰"""
        if self.is_warmed_up:
            print("æ¨¡å‹å·²ç»é¢„çƒ­ï¼Œè·³è¿‡é¢„çƒ­æ­¥éª¤")
            return

        warmup_start_time = time.time()
        dummy_input = np.random.rand(*self.input_shape).astype(np.float32)

        for _ in range(iterations):
            cuda.memcpy_htod_async(self.d_input, dummy_input, self.stream)
            self.context.execute_async_v3(stream_handle=self.stream.handle)
        self.stream.synchronize()

        warmup_end_time = time.time()
        warmup_time = (warmup_end_time - warmup_start_time) * 1000
        print(f"  é¢„çƒ­æ—¶é—´: {warmup_time:.2f} ms")
        self.is_warmed_up = True

    def infer(self, image) -> Tuple[float, np.ndarray]:
        self.cuda_context.push()
        try:
            """æ‰§è¡Œæ¨ç†ï¼ˆå¤ç”¨é¢„åˆ†é…æ˜¾å­˜ï¼‰"""
            if not self.is_warmed_up:
                print("è­¦å‘Šï¼šæ¨¡å‹å°šæœªé¢„çƒ­ï¼Œæ¨ç†æ€§èƒ½å¯èƒ½å—å½±å“")

            # é¢„å¤„ç†
            preprocess_time = time.time()
            input_data, r_p = preprocess(image)
            # print(f"é¢„å¤„ç†æ—¶é—´: {(time.time() - preprocess_time) * 1000:.2f} ms")

            # æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨è‡ªå·±çš„stream
            stream = cuda.Stream()

            # å¼‚æ­¥æ‹·è´æ•°æ®
            cuda.memcpy_htod_async(self.d_input, input_data, stream)

            # æ‰§è¡Œæ¨ç†
            self.context.execute_async_v3(stream_handle=stream.handle)

            # å¼‚æ­¥æ‹·è´ç»“æœå›ä¸»æœº
            cuda.memcpy_dtoh_async(self.h_output, self.d_output, stream)

            stream.synchronize()

            # åå¤„ç†
            postprocess_time = time.time()
            # self.h_output = torch.from_numpy(self.h_output).cuda()
            # results = non_max_suppression(self.h_output)
            # results = results.cpu().numpy()
            # or åå¤„ç†
            results = non_max_suppression_numpy(self.h_output)

            # è¿˜åŸåæ ‡åˆ°åŸå›¾
            results = scale_boxes(results, input_data, image, r_p)
            # print(f"åå¤„ç†æ—¶é—´ï¼š{(time.time() - postprocess_time) * 1000:.2f} ms")
            return results[0]

        finally:
            self.cuda_context.pop()

    def __del__(self):
        """ææ„å‡½æ•°è‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜"""
        if hasattr(self, 'd_input'):
            self.d_input.free()
        if hasattr(self, 'd_output'):
            self.d_output.free()
        print("æ˜¾å­˜èµ„æºå·²é‡Šæ”¾")

        """âœ… æ¸…ç†èµ„æºï¼Œé‡Šæ”¾ä¸Šä¸‹æ–‡"""
        # try:
        #     if hasattr(self, "context") and self.context:
        #         self.context.pop()
        #         del self.context
        # except Exception as e:
        #     print("Error cleaning up CUDA context:", e)


if __name__ == '__main__':
    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        trt_predictor = TensorRTPredictor(r"D:\PythonProject\Forward_Collision_Warning_System\models\engine\car_detector.engine")
        trt_predictor.warmup(10)

        # 1. æµ‹è¯•å•å¼ å›¾åƒæ¨ç†
        # image = cv2.imread("images/FILE250717-140233-000278F.jpg")
        # if image is not None:
        #     boxes = trt_predictor.infer(image)
        #     image = visualize(image, boxes)
        #     cv2.imshow('image', image)
        #     cv2.waitKey(0)
        #     cv2.destroyAllWindows()

        # 2. æµ‹è¯•è§†é¢‘æ¨ç†
        video_path = r'D:\PythonProject\Forward_Collision_Warning_System\video\test-1080p.mp4'
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            exit(1)

        count = 0
        total_time = 0

        print("å¼€å§‹è§†é¢‘æ¨ç†...")
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # æ¨ç†å…¨æµç¨‹
            t1 = time.time()
            boxes = trt_predictor.infer(frame)
            image = visualize(frame, boxes)
            t2 = time.time()

            frame_time = t2 - t1
            total_time += frame_time
            fps = 1 / frame_time if frame_time > 0 else 0

            print(f"æ¨ç†ç¬¬{count + 1}å¸§è€—æ—¶ï¼š{round(frame_time * 1000)}ms, FPS: {round(fps)}")

            # å¯é€‰ï¼šæ˜¾ç¤ºç»“æœ
            cv2.imshow('frame', image)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

            count += 1

            # å¯é€‰ï¼šé™åˆ¶å¤„ç†å¸§æ•°
            # if count >= 100:
            #     break

        cap.release()
        cv2.destroyAllWindows()

        # ç»Ÿè®¡ä¿¡æ¯
        if count > 0:
            avg_time = total_time / count
            avg_fps = 1 / avg_time if avg_time > 0 else 0
            print(f"\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»å¸§æ•°: {count}")
            print(f"å¹³å‡æ¯å¸§è€—æ—¶: {avg_time * 1000:.2f}ms")
            print(f"å¹³å‡FPS: {avg_fps:.2f}")






    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback

        traceback.print_exc()